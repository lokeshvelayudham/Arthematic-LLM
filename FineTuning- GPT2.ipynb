{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3be42097",
   "metadata": {},
   "source": [
    "# Fine-Tuning GPT-2 for Arithmetic Expression Prediction\n",
    "\n",
    "## Overview:\n",
    "- **Objective**: Fine-tune a GPT-2 model to predict results of arithmetic expressions.\n",
    "- **Steps**:\n",
    "  1. **Setup**: Imported libraries and prepared the tokenizer, model, and dataset.\n",
    "  2. **Data Handling**: Tokenized expressions and split the dataset for training, validation, and testing.\n",
    "  3. **Training**: Configured and fine-tuned GPT-2 using `Trainer`.\n",
    "  4. **Loss Tracking**: Implemented a custom callback to log training losses.\n",
    "  5. **Evaluation**: Tested the model on unseen data and displayed predictions.\n",
    "  6. **Visualization**: Plotted training loss trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f83aec8",
   "metadata": {},
   "source": [
    "## Import Libraries and Set Device\n",
    "\n",
    "- **Libraries Imported**:\n",
    "  - `torch` for PyTorch framework.\n",
    "  - `transformers` for using GPT-2 tokenizer and model.\n",
    "  - `torch.utils.data` for dataset handling.\n",
    "  - `os` for system path and file management.\n",
    "  \n",
    "- **GPU Check**:\n",
    "  - Determines if CUDA-compatible GPU is available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b035e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3de10192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8cb22a",
   "metadata": {},
   "source": [
    "## Dataset Preparation and Tokenization\n",
    "\n",
    "### Key Components:\n",
    "1. **Dataset Class**:\n",
    "   - `ArithmeticDataset`:\n",
    "     - Handles tokenization of input samples.\n",
    "     - Implements `__len__` and `__getitem__` for compatibility with PyTorch's `Dataset`.\n",
    "     - Sets `labels` as `input_ids` for GPT-2, which uses self-supervised learning.\n",
    "\n",
    "2. **Tokenizer Initialization**:\n",
    "   - GPT-2 tokenizer is loaded with `eos_token` as the padding token for compatibility.\n",
    "\n",
    "3. **Dataset Loading**:\n",
    "   - Reads a text file containing arithmetic samples.\n",
    "\n",
    "4. **Sample Preparation**:\n",
    "   - Combines consecutive lines into paired samples.\n",
    "\n",
    "5. **Dataset Splitting**:\n",
    "   - Splits samples into training (80%), validation (10%), and test (10%) sets using `train_test_split`.\n",
    "\n",
    "6. **Dataset Creation**:\n",
    "   - Converts the split samples into PyTorch datasets using the `ArithmeticDataset` class.\n",
    "\n",
    "### Outputs:\n",
    "- Number of samples in each split is displayed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d474a137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a08c6c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 479998\n",
      "Validation samples: 53334\n",
      "Test samples: 133334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "class ArithmeticDataset(Dataset):\n",
    "    def __init__(self, samples, tokenizer, max_length=512):\n",
    "        self.samples = samples\n",
    "\n",
    "        # Tokenize the dataset\n",
    "        self.tokenized_data = tokenizer(\n",
    "            self.samples,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.tokenized_data[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.tokenized_data[\"attention_mask\"][idx],\n",
    "            \"labels\": self.tokenized_data[\"input_ids\"][idx],  # GPT-2 uses input_ids as labels\n",
    "        }\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token for GPT-2 compatibility\n",
    "\n",
    "# Load the full dataset\n",
    "dataset_path = \"/home/lxp334/LLM_Final_Report/arithmetic__mixed.txt\"\n",
    "with open(dataset_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Prepare samples\n",
    "samples = [f\"{lines[i].strip()} {lines[i + 1].strip()}\" for i in range(0, len(lines), 2)]\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "train_samples, test_samples = train_test_split(samples, test_size=0.2, random_state=42)\n",
    "train_samples, val_samples = train_test_split(train_samples, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(train_samples)}\")\n",
    "print(f\"Validation samples: {len(val_samples)}\")\n",
    "print(f\"Test samples: {len(test_samples)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ArithmeticDataset(train_samples, tokenizer)\n",
    "val_dataset = ArithmeticDataset(val_samples, tokenizer)\n",
    "test_dataset = ArithmeticDataset(test_samples, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93c1697",
   "metadata": {},
   "source": [
    "## Custom Callback: Loss Logger\n",
    "\n",
    "### Purpose:\n",
    "- Tracks and logs the training loss during model training.\n",
    "\n",
    "### Implementation:\n",
    "1. **`LossLoggerCallback` Class**:\n",
    "   - Inherits from `TrainerCallback` in the `transformers` library.\n",
    "   - Maintains a list (`self.losses`) to store logged loss values.\n",
    "\n",
    "2. **`on_log` Method**:\n",
    "   - Triggered during logging events in training.\n",
    "   - Appends the current loss value (if available) from the `logs` dictionary to `self.losses`.\n",
    "\n",
    "### Usage:\n",
    "- This callback can be added to the `Trainer` to monitor loss trends across training epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad69cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "# Custom callback to track and log training loss\n",
    "class LossLoggerCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if \"loss\" in logs:\n",
    "            self.losses.append(logs[\"loss\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc0a9e2",
   "metadata": {},
   "source": [
    "## Model Training Setup\n",
    "\n",
    "- **Model**: Loaded GPT-2 and resized token embeddings to match tokenizer size.\n",
    "- **Training Arguments**:\n",
    "  - Checkpoints: Saved after each epoch in `./gpt2-arithmetic`.\n",
    "  - Epochs: 3, with a batch size of 8.\n",
    "  - Optimizer: Learning rate of `5e-5` with weight decay (`0.01`) and warm-up steps (`50`).\n",
    "  - Logging: Every 10 steps; logs saved in `./logs`.\n",
    "  - Mixed Precision: Enabled if GPU is available.\n",
    "  - Checkpoint Limit: Maximum of 2, with best model auto-loaded at end.\n",
    "\n",
    "- **Trainer**: Initialized with model, datasets, tokenizer, and `LossLoggerCallback` for tracking loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "282dced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e161e8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxp334/LLM_Final_Report/loki/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/job.2285984.hpc/ipykernel_1031537/2553319713.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Load the GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))  # Adjust the embedding size to match the tokenizer\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-arithmetic\",      # Directory to save model checkpoints\n",
    "    overwrite_output_dir=True,          # Overwrite existing output directory\n",
    "    num_train_epochs=3,                 # Number of training epochs\n",
    "    per_device_train_batch_size=8,      # Batch size per GPU/CPU\n",
    "    evaluation_strategy=\"epoch\",        # Evaluate the model at the end of each epoch\n",
    "    save_strategy=\"epoch\",              # Save the model at the end of each epoch\n",
    "    learning_rate=5e-5,                 # Learning rate\n",
    "    weight_decay=0.01,                  # Weight decay for regularization\n",
    "    warmup_steps=50,                    # Warm-up steps for learning rate scheduling\n",
    "    logging_dir=\"./logs\",               # Directory for training logs\n",
    "    logging_steps=10,                   # Log every 10 steps\n",
    "    save_total_limit=2,                 # Limit the number of saved checkpoints\n",
    "    load_best_model_at_end=True,        # Load the best model at the end of training\n",
    "    fp16=torch.cuda.is_available(),     # Use mixed-precision training if GPU is available\n",
    ")\n",
    "\n",
    "# Initialize the LossLoggerCallback\n",
    "loss_logger = LossLoggerCallback()\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[loss_logger]  # Attach the loss logger callback here\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619dc77",
   "metadata": {},
   "source": [
    "## Model Training and Loss Logging\n",
    "\n",
    "- **Training**: Model is trained using the configured `Trainer`.\n",
    "- **Loss Logging**:\n",
    "  - Saves tracked training losses from `LossLoggerCallback` to `training_losses.txt` for future reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed1c2cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180000' max='180000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180000/180000 1:40:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.855700</td>\n",
       "      <td>0.916585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.835100</td>\n",
       "      <td>0.883198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.785500</td>\n",
       "      <td>0.868684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the training losses for future reference\n",
    "with open(\"training_losses.txt\", \"w\") as f:\n",
    "    for loss in loss_logger.losses:\n",
    "        f.write(f\"{loss}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5317afaa",
   "metadata": {},
   "source": [
    "## Visualizing Training Loss\n",
    "\n",
    "- **Plot Configuration**:\n",
    "  - Loss plotted against training steps.\n",
    "  - Styled with Seaborn's \"whitegrid\" and pastel palette.\n",
    "  - Titles and axis labels added with enhanced styling.\n",
    "\n",
    "- **Output**:\n",
    "  - The loss plot is saved as `FineTuning_GPT2_Loss.png` with high resolution (300 DPI).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59e732ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /usr/local/easybuild_allnodes/software/matplotlib/3.5.2-foss-2022a/lib/python3.10/site-packages (3.5.2)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./loki/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./loki/lib/python3.10/site-packages (from matplotlib) (3.2.0)\n",
      "Collecting numpy>=1.23\n",
      "  Using cached numpy-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./loki/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/easybuild_allnodes/software/Pillow/9.1.1-GCCcore-11.3.0/lib/python3.10/site-packages (from matplotlib) (9.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/easybuild_allnodes/software/matplotlib/3.5.2-foss-2022a/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/easybuild_allnodes/software/matplotlib/3.5.2-foss-2022a/lib/python3.10/site-packages (from matplotlib) (4.34.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./loki/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/easybuild_allnodes/software/matplotlib/3.5.2-foss-2022a/lib/python3.10/site-packages (from matplotlib) (1.4.3)\n",
      "Requirement already satisfied: six>=1.5 in ./loki/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Installing collected packages: numpy, matplotlib\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.3\n",
      "    Not uninstalling numpy at /usr/local/easybuild_allnodes/software/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages, outside environment /home/lxp334/LLM_Final_Report/loki\n",
      "    Can't uninstall 'numpy'. No files were found to uninstall.\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.5.2\n",
      "    Not uninstalling matplotlib at /usr/local/easybuild_allnodes/software/matplotlib/3.5.2-foss-2022a/lib/python3.10/site-packages, outside environment /home/lxp334/LLM_Final_Report/loki\n",
      "    Can't uninstall 'matplotlib'. No files were found to uninstall.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scipy 1.8.1 requires numpy<1.25.0,>=1.17.3, but you have numpy 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed matplotlib-3.9.3 numpy-2.2.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/home/lxp334/LLM_Final_Report/loki/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install --upgrade matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "655bf845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set modern style for the plot\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
    "\n",
    "# Plot the training loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(\n",
    "    range(1, len(loss_logger.losses) + 1),\n",
    "    loss_logger.losses,\n",
    "    marker='o',\n",
    "    linestyle='-',\n",
    "    linewidth=2,\n",
    "    markersize=6,\n",
    "    label=\"Training Loss\",\n",
    ")\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title(\"Training Loss Over Steps\", fontsize=18, weight='bold')\n",
    "plt.xlabel(\"Training Steps\", fontsize=14, labelpad=10)\n",
    "plt.ylabel(\"Loss\", fontsize=14, labelpad=10)\n",
    "plt.legend(fontsize=12, loc='upper right')\n",
    "\n",
    "# Add grid and style\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"FineTuning_GPT2_Loss.png\", dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ddda43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully as fine_tuned_gpt2.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model as a .pth file\n",
    "torch.save(model.state_dict(), \"fine_tuned_gpt2.pth\")\n",
    "print(\"Model saved successfully as fine_tuned_gpt2.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe7fbaf",
   "metadata": {},
   "source": [
    "## Model Testing and Predictions\n",
    "\n",
    "- **Function**: `test_model_and_display_predictions`\n",
    "  - Evaluates the model on the test dataset and compares predictions with ground truth.\n",
    "\n",
    "- **Key Steps**:\n",
    "  1. **Evaluation Mode**: Model set to `eval` to disable gradient updates.\n",
    "  2. **Input Decoding**: Converts tokenized inputs back to text for readability.\n",
    "  3. **Prediction**: Generates output using greedy decoding.\n",
    "  4. **Ground Truth Comparison**: Assumes the last token in the input is the correct result.\n",
    "\n",
    "- **Output**:\n",
    "  - Displays a table of input expressions, ground truth values, and model predictions for a sample of test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7417b50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Results:\n",
      "Input Expression                                   Ground Truth    Prediction     \n",
      "--------------------------------------------------------------------------------\n",
      "What is the value of (-9)/12 + 0 - 38/(-8)? 4      4               4              \n",
      "What is 500/(-40) - (-27)/6? -8                    -8              -8             \n",
      "What is the value of 1/(-5) + (-152)/190? -1       -1              -1             \n",
      "Evaluate (4/2)/(-3*(-42)/(-18)). -2/7              -2/7            -2/7           \n",
      "Calculate (-12)/40*(102/18 - 5). -1/5              -1/5            -1/5           \n",
      "What is 6/(-120)*6 - (-2)/5? 1/10                  1/10            1/10           \n",
      "Evaluate 8*1/((-9)/((-18)/8)). 2                   2               2              \n",
      "What is the value of (-12)/15 + (-357)/(-315)? 1/3 1/3             1/3            \n",
      "Calculate (288/320)/(6/(-10)) - 1. -5/2            -5/2            -5/2           \n",
      "What is the value of 8/3*(18/(-12) - 0)? -4        -4              -4             \n"
     ]
    }
   ],
   "source": [
    "def test_model_and_display_predictions(test_dataset, model, tokenizer, num_examples=10):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    inputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(min(num_examples, len(test_dataset))):\n",
    "            sample = test_dataset[idx]\n",
    "\n",
    "            # Decode input sequence\n",
    "            input_text = tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=True)\n",
    "            inputs.append(input_text)\n",
    "\n",
    "            # Extract ground truth\n",
    "            ground_truth = input_text.split()[-1]  # Assuming the last token is the result\n",
    "            ground_truths.append(ground_truth)\n",
    "\n",
    "            # Generate prediction\n",
    "            generated = model.generate(\n",
    "                input_ids=sample[\"input_ids\"].unsqueeze(0).to(device),\n",
    "                max_length=50,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                num_beams=1  # Greedy decoding\n",
    "            )\n",
    "            predicted_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "            predicted_result = predicted_text.split()[-1]  # Extract predicted result\n",
    "            predictions.append(predicted_result)\n",
    "\n",
    "    # Display results\n",
    "    print(\"Testing Results:\")\n",
    "    print(f\"{'Input Expression':<50} {'Ground Truth':<15} {'Prediction':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    for inp, gt, pred in zip(inputs, ground_truths, predictions):\n",
    "        print(f\"{inp:<50} {gt:<15} {pred:<15}\")\n",
    "        \n",
    "# Test the fine-tuned model and display predictions\n",
    "test_model_and_display_predictions(test_dataset, model, tokenizer, num_examples=10)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (loki)",
   "language": "python",
   "name": "loki"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
