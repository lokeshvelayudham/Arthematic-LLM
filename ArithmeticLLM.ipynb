{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc79d01a",
   "metadata": {},
   "source": [
    "# Building and Fine-Tuning a Transformer-Based LLM for Arithmetic\n",
    "\n",
    "## Overview:\n",
    "- **Objective**: Develop a custom transformer-based Language Model (LLM) from scratch to predict results of arithmetic expressions.\n",
    "- **Steps**:\n",
    "  1. **Custom Model Design**: Implemented a transformer model with positional encoding and dynamic sequence alignment.\n",
    "  2. **Dataset Creation**: Generated synthetic arithmetic datasets, tokenized, and split for training, validation, and testing.\n",
    "  3. **Pretraining**: Masked token pretraining for foundational learning.\n",
    "  4. **Fine-Tuning**: Trained the model to improve performance on specific tasks with learning rate scheduling and early stopping.\n",
    "  5. **Evaluation**: Tested the LLM on unseen data, comparing predictions to expected results.\n",
    "\n",
    "## Output:\n",
    "- Built and fine-tuned a transformer-based LLM for arithmetic tasks.\n",
    "- Visualized training progress and evaluated model performance with detailed results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f853d3",
   "metadata": {},
   "source": [
    "# Setup: Import Libraries and Configure Device\n",
    "\n",
    "## Libraries Imported:\n",
    "- **PyTorch**: For building and training neural networks.\n",
    "- **Transformers**: For optimization (`AdamW`) and learning rate scheduling (`get_scheduler`).\n",
    "- **Scikit-learn**: For dataset splitting.\n",
    "- **Random**: For reproducibility.\n",
    "\n",
    "## Device Configuration:\n",
    "- Checks for CUDA availability and sets the device to GPU (`cuda`) if available, otherwise CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0806fbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW, get_scheduler\n",
    "import random\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52786f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "Number of GPUs: 1\n",
      "GPU 0: NVIDIA L40S\n",
      "  Memory Allocated: 0.00 MB\n",
      "  Memory Cached: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "    # Display details of each GPU\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"  Memory Allocated: {torch.cuda.memory_allocated(i) / 1024 ** 2:.2f} MB\")\n",
    "        print(f\"  Memory Cached: {torch.cuda.memory_reserved(i) / 1024 ** 2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310f7141",
   "metadata": {},
   "source": [
    "## Custom Tokenizer: ArithmeticTokenizer\n",
    "\n",
    "### Purpose:\n",
    "- Tokenizes arithmetic expressions into indices and decodes indices back into expressions.\n",
    "\n",
    "### Key Features:\n",
    "- **Vocabulary**:\n",
    "  - Includes digits (`0-9`), operators (`+-*/`), parentheses, equals sign (`=`), space, `<PAD>`, and `<MASK>`.\n",
    "- **Methods**:\n",
    "  - `encode`: Converts a string into a sequence of token indices.\n",
    "  - `decode`: Converts token indices back to a string, ignoring `<PAD>` tokens.\n",
    "\n",
    "### Initialization:\n",
    "- Instantiated as `tokenizer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e7c4012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Initialized\n"
     ]
    }
   ],
   "source": [
    "# Define a tokenizer\n",
    "class ArithmeticTokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = {char: idx for idx, char in enumerate(\"0123456789+-*/().= \")}\n",
    "        self.vocab[\"<PAD>\"] = len(self.vocab)\n",
    "        self.vocab[\"<MASK>\"] = len(self.vocab)\n",
    "        self.reverse_vocab = {idx: char for char, idx in self.vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.vocab.get(char, self.vocab[\"<PAD>\"]) for char in text]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return \"\".join([self.reverse_vocab[idx] for idx in indices if idx in self.reverse_vocab and idx != self.vocab[\"<PAD>\"]])\n",
    "\n",
    "# Instantiate tokenizer\n",
    "tokenizer = ArithmeticTokenizer()\n",
    "\n",
    "print(\"Tokenizer Initialized\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28854f68",
   "metadata": {},
   "source": [
    "## Synthetic Arithmetic Expression Generation\n",
    "\n",
    "### Overview:\n",
    "- **Objective**: Create a dataset of synthetic arithmetic expressions for pre-training.\n",
    "\n",
    "### Key Features:\n",
    "1. **Operators**: `+`, `-`, `*`, `/`.\n",
    "2. **Expression Count**: Generated `50,000` expressions.\n",
    "3. **Value Range**: Operands range between `1` and `100`.\n",
    "\n",
    "### Methods:\n",
    "1. **`generate_expression(max_depth)`**:\n",
    "   - Recursively builds random arithmetic expressions with adjustable complexity (`max_depth`).\n",
    "   - Base case generates a single random number.\n",
    "2. **`generate_sample_expression()`**:\n",
    "   - Wraps expressions in natural-language prompts (e.g., \"Calculate (2 + 3).\").\n",
    "\n",
    "### Dataset:\n",
    "- **Size**: 50,000 expressions.\n",
    "- **Sample Output**:\n",
    "  - Example prompts printed for verification.\n",
    "- **Storage**: Saved expressions to `pretraining_data.txt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea99d499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 50000 expressions for pre-training.\n",
      "Sample expressions:\n",
      "Evaluate (79 - 54).\n",
      "Calculate ((11 / 42) * (100 - 28)).\n",
      "What is ((3 + 46) + (80 / 81))?\n",
      "What is (71 - 28)?\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic arithmetic expressions\n",
    "operators = ['+', '-', '*', '/']\n",
    "num_expressions = 50000  # Number of expressions to generate\n",
    "min_value, max_value = 1, 100  # Range for numbers\n",
    "\n",
    "def generate_expression(max_depth=2):\n",
    "    \"\"\"Generate random arithmetic expressions with adjustable complexity.\"\"\"\n",
    "    if max_depth == 0:\n",
    "        # Base case: return a single number\n",
    "        return str(random.randint(min_value, max_value))\n",
    "    else:\n",
    "        left = generate_expression(max_depth - 1)\n",
    "        right = generate_expression(max_depth - 1)\n",
    "        operator = random.choice(operators)\n",
    "        return f\"({left} {operator} {right})\"\n",
    "\n",
    "def generate_sample_expression():\n",
    "    \"\"\"Generate expressions with sample-like formatting.\"\"\"\n",
    "    # Avoid generating single-number queries\n",
    "    expression = generate_expression(max_depth=random.randint(1, 2))\n",
    "    \n",
    "    # Use prompts only for valid arithmetic expressions\n",
    "    prompt_templates = [\n",
    "        \"Calculate {}.\",\n",
    "        \"Evaluate {}.\",\n",
    "        \"What is {}?\"\n",
    "    ]\n",
    "    prompt = random.choice(prompt_templates)\n",
    "    return prompt.format(expression)\n",
    "\n",
    "# Generate dataset\n",
    "pretraining_data = [generate_sample_expression() for _ in range(num_expressions)]\n",
    "print(f\"Generated {len(pretraining_data)} expressions for pre-training.\")\n",
    "\n",
    "# Print a few samples for verification\n",
    "print(\"Sample expressions:\")\n",
    "for i in range(4):\n",
    "    print(pretraining_data[i])\n",
    "\n",
    "# Save dataset to a file for reference\n",
    "with open(\"pretraining_data.txt\", \"w\") as f:\n",
    "    for expr in pretraining_data:\n",
    "        f.write(expr + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404fc335",
   "metadata": {},
   "source": [
    "## Transformer Model for Arithmetic Expressions\n",
    "\n",
    "### Components:\n",
    "1. **`PositionalEncoding`**:\n",
    "   - Adds learnable positional encoding to embeddings.\n",
    "   - Ensures model captures positional relationships in input sequences.\n",
    "\n",
    "2. **`initialize_weights`**:\n",
    "   - Initializes model weights:\n",
    "     - `nn.Linear` and `nn.Embedding`: Xavier uniform initialization.\n",
    "     - `nn.LayerNorm`: Bias to `0`, weight to `1`.\n",
    "\n",
    "3. **`TransformerArithmeticModel`**:\n",
    "   - **Embedding Layer**: Maps token indices to dense vectors (`d_model` size).\n",
    "   - **Positional Encoding**: Adds positional information to embeddings.\n",
    "   - **Transformer**: Core transformer architecture with encoder-decoder layers.\n",
    "   - **Output Layer**: Projects transformer outputs to vocabulary size.\n",
    "   - **Methods**:\n",
    "     - `forward`: Processes source and target sequences for sequence-to-sequence training.\n",
    "     - `pretrain_forward`: Processes source sequences for pretraining tasks.\n",
    "\n",
    "### Initialization:\n",
    "- Model instantiated with vocabulary size matching the custom tokenizer.\n",
    "- Weights initialized using `initialize_weights`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "125fd50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxp334/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Model Initialized\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = nn.Parameter(torch.zeros(1, max_len, d_model), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1), :]\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "            nn.init.constant_(module.weight, 1.0)\n",
    "\n",
    "class TransformerArithmeticModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=6, dropout=0.1):\n",
    "        super(TransformerArithmeticModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model, nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers,\n",
    "            dropout=dropout, batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Handles source and target sequences for training.\n",
    "        \"\"\"\n",
    "        # Embed and encode source and target\n",
    "        src_emb = self.dropout(self.positional_encoding(self.embedding(src)))\n",
    "        tgt_emb = self.dropout(self.positional_encoding(self.embedding(tgt)))\n",
    "\n",
    "        # Pass through transformer\n",
    "        transformer_output = self.transformer(src_emb, tgt_emb)\n",
    "\n",
    "        # Project to vocabulary size\n",
    "        return self.fc_out(transformer_output)\n",
    "\n",
    "    def pretrain_forward(self, src):\n",
    "        src_emb = self.dropout(self.positional_encoding(self.embedding(src)))\n",
    "        output = self.transformer(src_emb, src_emb)\n",
    "        return self.fc_out(output)\n",
    "    \n",
    "\n",
    "model = TransformerArithmeticModel(vocab_size=len(tokenizer.vocab)).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "initialize_weights(model)\n",
    "print(\"Transformer Model Initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ae67f",
   "metadata": {},
   "source": [
    "## Pretraining Custom Transformer Model on Arithmetic Expressions\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Dataset Preparation**:\n",
    "   - `PreTrainingDataset`:\n",
    "     - Converts arithmetic expressions into tokenized sequences.\n",
    "     - Handles batching and padding using `DataLoader` with `<PAD>` token.\n",
    "   - Dataset size: 50,000 expressions.\n",
    "\n",
    "2. **Token Masking**:\n",
    "   - **`mask_tokens`**:\n",
    "     - Masks 15% of tokens in input sequences with `<MASK>` for pretraining.\n",
    "     - Ensures at least one unmasked token remains per sequence.\n",
    "     - Labels unmasked tokens for loss calculation.\n",
    "\n",
    "3. **Model Pretraining**:\n",
    "   - **Optimization**:\n",
    "     - Uses `AdamW` optimizer with learning rate `5e-4` and gradient clipping.\n",
    "   - **Loss Function**:\n",
    "     - `CrossEntropyLoss` with ignore index `-100` for masked tokens.\n",
    "   - **Training**:\n",
    "     - Pretrained for 5 epochs with validation for `NaN`/`Inf` errors in inputs, outputs, and gradients.\n",
    "\n",
    "4. **Loss Visualization**:\n",
    "   - Plots pretraining loss across epochs.\n",
    "\n",
    "### Outputs:\n",
    "- **Console**: Logs average loss per epoch.\n",
    "- **Visualization**: A plot of pretraining loss saved and optionally displayed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28afd600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and DataLoader Initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxp334/.local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.9126\n",
      "Epoch 2/5, Loss: 0.5239\n",
      "Epoch 3/5, Loss: 0.4999\n",
      "Epoch 4/5, Loss: 0.4853\n",
      "Epoch 5/5, Loss: 0.4889\n",
      "Pretraining Loss Saved to /home/lxp334/LLM_Final_Report/visualizations/ArithmeticLLM_Pretraining_Loss_Visualization.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/job.2285984.hpc/ipykernel_1013567/3549464330.py:121: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "class PreTrainingDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = [torch.tensor(tokenizer.encode(expr)) for expr in data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data[idx]\n",
    "        return seq\n",
    "\n",
    "dataset = PreTrainingDataset(pretraining_data, tokenizer)\n",
    "pretrain_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=lambda x: pad_sequence(x, batch_first=True, padding_value=tokenizer.vocab[\"<PAD>\"]))\n",
    "print(\"Dataset and DataLoader Initialized\")\n",
    "\n",
    "\n",
    "\n",
    "def mask_tokens(input_seq, mask_token, mask_prob=0.15):\n",
    "    \"\"\"Masks a percentage of tokens in the input sequence for pre-training.\"\"\"\n",
    "    masked_seq = input_seq.clone()\n",
    "    labels = input_seq.clone()\n",
    "\n",
    "    # Mask tokens with the given probability\n",
    "    for i in range(input_seq.size(1)):\n",
    "        if random.random() < mask_prob:\n",
    "            masked_seq[:, i] = mask_token\n",
    "        else:\n",
    "            labels[:, i] = -100  # Ignore index for loss\n",
    "\n",
    "    # Ensure at least one unmasked token per sequence\n",
    "    for i in range(masked_seq.size(0)):\n",
    "        if torch.all(labels[i] == -100):\n",
    "            random_idx = random.randint(0, masked_seq.size(1) - 1)\n",
    "            labels[i, random_idx] = input_seq[i, random_idx]\n",
    "            masked_seq[i, random_idx] = input_seq[i, random_idx]\n",
    "\n",
    "    return masked_seq, labels\n",
    "\n",
    "def pretrain_model(model, dataloader, tokenizer, epochs=5, lr=5e-4):\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            try:\n",
    "                # Move batch to device\n",
    "                batch = batch.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "                # Mask tokens\n",
    "                masked_seq, labels = mask_tokens(batch, mask_token=tokenizer.vocab[\"<MASK>\"])\n",
    "                masked_seq, labels = masked_seq.to(\"cuda\" if torch.cuda.is_available() else \"cpu\"), labels.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "                # Validate inputs\n",
    "                if torch.isnan(masked_seq).any() or torch.isnan(labels).any():\n",
    "                    print(f\"NaN detected in masked_seq or labels at batch {batch_idx}. Skipping batch.\")\n",
    "                    continue\n",
    "\n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                output = model.pretrain_forward(masked_seq)\n",
    "                \n",
    "                # Validate outputs\n",
    "                if torch.isnan(output).any() or torch.isinf(output).any():\n",
    "                    print(f\"NaN or Inf detected in model output at batch {batch_idx}. Skipping batch.\")\n",
    "                    continue\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(output.view(-1, output.size(-1)), labels.view(-1))\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"NaN detected in loss at batch {batch_idx}. Skipping batch.\")\n",
    "                    continue\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    import matplotlib.pyplot as plt  \n",
    "\n",
    "    # Enhanced plot appearance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(\n",
    "        range(1, len(losses) + 1), \n",
    "        losses, \n",
    "        marker='o', \n",
    "        linestyle='-', \n",
    "        linewidth=2, \n",
    "        markersize=10,\n",
    "        label='Loss'\n",
    "    )\n",
    "\n",
    "    # Modern aesthetics\n",
    "    plt.title(\"Pretraining Loss\", fontsize=20, fontweight='bold', loc='center')\n",
    "    plt.xlabel(\"Epochs\", fontsize=16)\n",
    "    plt.ylabel(\"Loss\", fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.legend(fontsize=14, loc='upper right')\n",
    "    plt.grid(which='both', linestyle='--', linewidth=0.7, alpha=0.7)\n",
    "    plt.tight_layout()  # Ensure layout fits well\n",
    "\n",
    "    # Save the plot\n",
    "    output_path = \"/home/lxp334/LLM_Final_Report/visualizations/ArithmeticLLM_Pretraining_Loss_Visualization.png\"\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "\n",
    "    # Show the plot for immediate feedback (optional)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Pretraining Loss Saved to {output_path}\")\n",
    "\n",
    "\n",
    "pretrain_model(model, pretrain_loader, tokenizer, epochs=5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c989e70",
   "metadata": {},
   "source": [
    "## Loading and Tokenizing Multiple Datasets\n",
    "\n",
    "### Function: `load_multiple_datasets`\n",
    "- **Purpose**: \n",
    "  - Reads arithmetic datasets from specified file paths.\n",
    "  - Tokenizes arithmetic expressions and their results.\n",
    "\n",
    "### Steps:\n",
    "1. **File Reading**:\n",
    "   - Reads each dataset line by line.\n",
    "   - Pairs consecutive lines as `expression` (line `i`) and `result` (line `i+1`).\n",
    "2. **Tokenization**:\n",
    "   - Encodes both `expression` and `result` using the custom `ArithmeticTokenizer`.\n",
    "3. **Dataset Combination**:\n",
    "   - Combines all datasets into a single list of tokenized pairs.\n",
    "\n",
    "### Outputs:\n",
    "- Total number of tokenized samples logged.\n",
    "- Debug: Prints the first encoded sample for verification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98954350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: /home/lxp334/LLM_Final_Report/arithmetic__mixed.txt\n",
      "Total samples from all datasets: 666666\n",
      "Total Samples: 666666\n",
      "Sample Data (Encoded): [([1, 5, 18, 10, 18, 14, 7, 18, 10, 18, 11, 1, 7, 15, 13, 1], [5])]\n"
     ]
    }
   ],
   "source": [
    "def load_multiple_datasets(file_paths, tokenizer):\n",
    "    \"\"\"Loads and tokenizes multiple datasets.\"\"\"\n",
    "    all_data = []\n",
    "    for file_path in file_paths:\n",
    "        print(f\"Loading dataset from: {file_path}\")\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for i in range(0, len(lines), 2):\n",
    "                expression = lines[i].strip()\n",
    "                result = lines[i + 1].strip()\n",
    "                all_data.append((tokenizer.encode(expression), tokenizer.encode(result)))\n",
    "    print(f\"Total samples from all datasets: {len(all_data)}\")\n",
    "    return all_data\n",
    "\n",
    "# File paths for datasets\n",
    "file_paths = [\n",
    "    \"/home/lxp334/LLM_Final_Report/arithmetic__mixed.txt\",\n",
    "]\n",
    "\n",
    "# Load and combine datasets\n",
    "combined_data = load_multiple_datasets(file_paths, tokenizer)\n",
    "\n",
    "# Debug: Print dataset stats and a sample\n",
    "print(f\"Total Samples: {len(combined_data)}\")\n",
    "print(f\"Sample Data (Encoded): {combined_data[:1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94964cf3",
   "metadata": {},
   "source": [
    "## Splitting Dataset into Training, Validation, and Test Sets\n",
    "\n",
    "### Process:\n",
    "1. **Dataset Splitting**:\n",
    "   - **Training Set**: 72% of the dataset.\n",
    "   - **Validation Set**: 8% of the dataset.\n",
    "   - **Test Set**: 20% of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52db9a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Samples: 479998\n",
      "Validation Samples: 53334\n",
      "Test Samples: 133334\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "train_data, test_data = train_test_split(combined_data, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n",
    "\n",
    "# Debug: Print split sizes\n",
    "print(f\"Train Samples: {len(train_data)}\")\n",
    "print(f\"Validation Samples: {len(val_data)}\")\n",
    "print(f\"Test Samples: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c757d18",
   "metadata": {},
   "source": [
    "## Fine-Tuning Dataset and DataLoaders\n",
    "\n",
    "### Components:\n",
    "1. **`FineTuningDataset` Class**:\n",
    "   - Wraps the dataset (`train_data`, `val_data`, `test_data`) for use with PyTorch.\n",
    "   - Implements `__len__` and `__getitem__`.\n",
    "\n",
    "2. **`collate_fn`**:\n",
    "   - Pads input expressions and results to the same length within a batch.\n",
    "   - Uses the `<PAD>` token from the custom tokenizer.\n",
    "\n",
    "3. **DataLoaders**:\n",
    "   - Created for training, validation, and testing:\n",
    "     - **Training DataLoader**: Shuffles batches for randomness.\n",
    "     - **Validation and Test DataLoaders**: No shuffling.\n",
    "   - Batch size set to `32`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89bd1646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shapes - Expressions: torch.Size([32, 51]) Results: torch.Size([32, 5])\n",
      "FineTuner Initialized\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Fine-tuning dataset\n",
    "class FineTuningDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Collate function for padding\n",
    "def collate_fn(batch):\n",
    "    expressions, results = zip(*batch)\n",
    "    padded_expressions = pad_sequence([torch.tensor(expr) for expr in expressions], batch_first=True, padding_value=tokenizer.vocab[\"<PAD>\"])\n",
    "    padded_results = pad_sequence([torch.tensor(res) for res in results], batch_first=True, padding_value=tokenizer.vocab[\"<PAD>\"])\n",
    "    return padded_expressions, padded_results\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(FineTuningDataset(train_data), batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "val_loader = DataLoader(FineTuningDataset(val_data), batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(FineTuningDataset(test_data), batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# Debug: Check a batch\n",
    "for src, tgt in train_loader:\n",
    "    print(\"Batch Shapes - Expressions:\", src.shape, \"Results:\", tgt.shape)\n",
    "    break\n",
    "    \n",
    "print(\"FineTuner Initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09778b90",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Transformer Model\n",
    "\n",
    "### Training and Validation:\n",
    "1. **Function**: `train_finetune_model`\n",
    "   - Handles training and validation of the model with early stopping.\n",
    "   - Tracks training and validation losses for visualization.\n",
    "\n",
    "2. **Key Features**:\n",
    "   - **Dynamic Sequence Alignment**: Aligns source (`src`) and target (`tgt`) sequence lengths.\n",
    "   - **Criterion**: Uses `CrossEntropyLoss` with `<PAD>` token ignored.\n",
    "   - **Optimizer**: AdamW optimizer with a learning rate of `5e-4`.\n",
    "   - **Scheduler**: Linear learning rate scheduler with warm-up steps.\n",
    "   - **Early Stopping**: Stops training if validation loss does not improve for `5` epochs.\n",
    "\n",
    "3. **Output**:\n",
    "   - Logs epoch number, training loss, validation loss, and epoch duration.\n",
    "   - Returns training and validation losses for visualization.\n",
    "\n",
    "### Visualization:\n",
    "1. **Function**: `plot_finetuning_loss`\n",
    "   - Plots training and validation loss trends over epochs.\n",
    "\n",
    "### Execution:\n",
    "- Trains the model using `train_loader` and validates with `val_loader`.\n",
    "- Plots and saves the loss graph for analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b68ab1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxp334/.local/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 7.9884, Val Loss: 8.0693, Time: 282.21 seconds\n",
      "Epoch 2/5, Train Loss: 1.2155, Val Loss: 0.2969, Time: 281.75 seconds\n",
      "Epoch 3/5, Train Loss: 0.2137, Val Loss: 0.1168, Time: 281.20 seconds\n",
      "Epoch 4/5, Train Loss: 0.1253, Val Loss: 0.1097, Time: 281.31 seconds\n",
      "Epoch 5/5, Train Loss: 0.1129, Val Loss: 0.1086, Time: 281.41 seconds\n",
      "Loss graph saved as /home/lxp334/LLM_Final_Report/visualizations/ArithmeticLLM_Finetuning_Loss_Visualization.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/job.2285984.hpc/ipykernel_1013567/590856479.py:109: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def train_finetune_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=5, patience=5):\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()  # Start timer for the epoch\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for src, tgt in train_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            # Align `src` length with `tgt` length\n",
    "            max_tgt_len = tgt.size(1)  # Length of target sequences\n",
    "            src = src[:, :max_tgt_len]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(src, tgt[:, :-1])\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output.reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for src, tgt in val_loader:\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "                # Align `src` length with `tgt` length\n",
    "                src = src[:, :tgt.size(1)]\n",
    "\n",
    "                output = model(src, tgt[:, :-1])\n",
    "                loss = criterion(output.reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        epoch_end_time = time.time()  # End timer for the epoch\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Time: {epoch_duration:.2f} seconds\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Stopping early due to no improvement.\")\n",
    "                break\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_finetuning_loss(train_losses, val_losses):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss over epochs with a modern and polished look.\n",
    "    \"\"\"\n",
    "    epochs = list(range(1, len(train_losses) + 1))\n",
    "    \n",
    "    # Modern style plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(\n",
    "        epochs, train_losses, label=\"Training Loss\", \n",
    "        marker='o', linestyle='-', linewidth=2.5, markersize=10, color='#1f77b4'\n",
    "    )\n",
    "    plt.plot(\n",
    "        epochs, val_losses, label=\"Validation Loss\", \n",
    "        marker='s', linestyle='--', linewidth=2.5, markersize=10, color='#ff7f0e'\n",
    "    )\n",
    "\n",
    "    # Title and labels with enhanced styling\n",
    "    plt.title(\"Fine-Tuning Loss Visualization\", fontsize=20, fontweight='bold', loc='center')\n",
    "    plt.xlabel(\"Epochs\", fontsize=16)\n",
    "    plt.ylabel(\"Loss\", fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "    # Add legend and grid\n",
    "    plt.legend(fontsize=14, loc='upper right', frameon=True, shadow=True)\n",
    "    plt.grid(which='both', linestyle='--', linewidth=0.7, alpha=0.7)\n",
    "\n",
    "    # Adjust layout for better fit\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    output_path = \"/home/lxp334/LLM_Final_Report/visualizations/ArithmeticLLM_Finetuning_Loss_Visualization.png\"\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "\n",
    "    # Optional: Show the plot (for debugging or immediate visualization)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Loss graph saved as {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.vocab[\"<PAD>\"])\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=100, num_training_steps=len(train_loader) * 50)\n",
    "\n",
    "train_losses, val_losses = train_finetune_model(model, train_loader, val_loader, criterion, optimizer, scheduler)\n",
    "\n",
    "plot_finetuning_loss(train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b03e8",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "- **Function**: `evaluate_model`\n",
    "  - Runs the model on test data, decodes inputs, targets, and predictions.\n",
    "  - Stores results as (`Expression`, `Expected`, `Predicted`).\n",
    "\n",
    "- **Output**:\n",
    "  - First 15 results printed for verification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "875d6191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expression:      (-9)/12 + 0 - 38/(-8) | Expected: 4 | Predicted: 444\n",
      "Expression:   500/(-40) - (-27)/6 | Expected: -8 | Predicted: 8/7\n",
      "Expression:      1/(-5) + (-152)/190 | Expected: -1 | Predicted: 114\n",
      "Expression:  (4/2)/(-3*(-42)/(-18)). | Expected: -2/7 | Predicted: 2/5\n",
      "Expression:  (-12)/40*(102/18 - 5). | Expected: -1/5 | Predicted: 1/3\n",
      "Expression:   6/(-120)*6 - (-2)/5 | Expected: 1/10 | Predicted: /11\n",
      "Expression:  8*1/((-9)/((-18)/8)). | Expected: 2 | Predicted: /44\n",
      "Expression:      (-12)/15 + (-357)/(-315) | Expected: 1/3 | Predicted: /31\n",
      "Expression:  (288/320)/(6/(-10)) - 1. | Expected: -5/2 | Predicted: 5/3\n",
      "Expression:      8/3*(18/(-12) - 0) | Expected: -4 | Predicted: 4/5\n",
      "Expression:  5*3/9*-3. | Expected: -5 | Predicted: 5/5\n",
      "Expression:      (2 + 2)/(-4 + (-40)/(-12)) | Expected: -6 | Predicted: 617\n",
      "Expression:  ((-12)/(-27))/(134/603). | Expected: 2 | Predicted: /44\n",
      "Expression:      (-21)/(-3) + (-46)/8 | Expected: 5/4 | Predicted: /45\n",
      "Expression:   11 + -7 + 42/16 + -7 | Expected: -3/8 | Predicted: 3/2\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, tokenizer):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in test_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            predictions = torch.argmax(output, dim=2)\n",
    "            for i in range(src.size(0)):\n",
    "                src_decoded = tokenizer.decode([t for t in src[i].tolist() if t != tokenizer.vocab[\"<PAD>\"]])\n",
    "                tgt_decoded = tokenizer.decode([t for t in tgt[i].tolist() if t != tokenizer.vocab[\"<PAD>\"]])\n",
    "                pred_decoded = tokenizer.decode([t for t in predictions[i].tolist() if t != tokenizer.vocab[\"<PAD>\"]])\n",
    "                results.append((src_decoded, tgt_decoded, pred_decoded))\n",
    "    return results\n",
    "\n",
    "test_results = evaluate_model(model, test_loader, tokenizer)\n",
    "for src, tgt, pred in test_results[:15]:\n",
    "    print(f\"Expression: {src} | Expected: {tgt} | Predicted: {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeae9e1",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2ec3213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"ArithmetiLLM_fineTuned_1DS.pth\")\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GCCcore-11.3.0)",
   "language": "python",
   "name": "gcccore-11.3.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
